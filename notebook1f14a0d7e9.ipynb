{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":168.09806,"end_time":"2023-09-13T18:58:13.013821","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-09-13T18:55:24.915761","version":"2.4.0"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3404,"sourceType":"datasetVersion","datasetId":1985}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" This dataset contains all transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\nThis notebook consists of the below:\n- Data cleaning\n- EDA\n- Feature Engineering with RFM\n- Outlier treatment with Isolation Forest\n- KMeans clustering\n- Visualization of clusters with multiple displays\n- Selection of products to display to each customer segment as they traverse the web site:\n    * Top 5 most purchased products per customer segment\n    * For customers that purchased the top product in each customer segment, the top 3 most popular items are determined i.e. \"Customers that purchased this also purchased...\"\n","metadata":{"id":"ceddxA2MEk91"}},{"cell_type":"code","source":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport plotly.graph_objects as go\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib import colors as mcolors\nfrom scipy.stats import linregress\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.cluster import KMeans\nfrom tabulate import tabulate\nfrom collections import Counter\n\n%matplotlib inline","metadata":{"papermill":{"duration":3.71761,"end_time":"2023-09-13T18:55:33.705058","exception":false,"start_time":"2023-09-13T18:55:29.987448","status":"completed"},"tags":[],"id":"88c0d4f2","execution":{"iopub.status.busy":"2025-03-17T14:48:55.822249Z","iopub.execute_input":"2025-03-17T14:48:55.826110Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Plotly for use in the notebook\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)","metadata":{"papermill":{"duration":0.326571,"end_time":"2023-09-13T18:55:34.104602","exception":false,"start_time":"2023-09-13T18:55:33.778031","status":"completed"},"tags":[],"id":"6f2a57e4","outputId":"dd04f40b-cbce-4192-f003-03b03b0665c4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure Seaborn plot styles: Set background color and use dark grid\nsns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')","metadata":{"papermill":{"duration":0.065754,"end_time":"2023-09-13T18:55:34.225966","exception":false,"start_time":"2023-09-13T18:55:34.160212","status":"completed"},"tags":[],"id":"e7f647ef","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ecommerce-data/data.csv', encoding=\"ISO-8859-1\")","metadata":{"papermill":{"duration":1.815595,"end_time":"2023-09-13T18:55:36.317782","exception":false,"start_time":"2023-09-13T18:55:34.502187","status":"completed"},"tags":[],"id":"baaa0be4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:130%; text-align:left\">\n\n<h2 align=\"left\"><font color=#ff6200>Dataset Description:</font></h2>\n\n| __Variable__   | __Description__ |\n|     :---       |       :---      |      \n| __InvoiceNo__  | Code representing each unique transaction.  If this code starts with letter 'c', it indicates a cancellation. |\n| __StockCode__  | Code uniquely assigned to each distinct product. |\n| __Description__| Description of each product. |\n| __Quantity__   | The number of units of a product in a transaction. |\n| __InvoiceDate__| The date and time of the transaction. |\n| __UnitPrice__  | The unit price of the product in sterling. |\n| __CustomerID__ | Identifier uniquely assigned to each customer. |\n| __Country__    | The country of the customer. |\n","metadata":{"papermill":{"duration":0.055908,"end_time":"2023-09-13T18:55:36.429299","exception":false,"start_time":"2023-09-13T18:55:36.373391","status":"completed"},"tags":[],"id":"53887068"}},{"cell_type":"code","source":"df.head(10)","metadata":{"papermill":{"duration":0.098838,"end_time":"2023-09-13T18:55:37.031877","exception":false,"start_time":"2023-09-13T18:55:36.933039","status":"completed"},"tags":[],"id":"4cec4265","outputId":"f3e71386-173a-4355-a671-4578a45eb6f6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"papermill":{"duration":0.367369,"end_time":"2023-09-13T18:55:37.454997","exception":false,"start_time":"2023-09-13T18:55:37.087628","status":"completed"},"tags":[],"id":"d408d719","outputId":"2196a02b-2e1a-47ba-8ff0-d5ebe1ba000b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Description and CustomerID has some missing values.","metadata":{"id":"NZUQLoF08cc4"}},{"cell_type":"code","source":"# Summary statistics for numerical variables\ndf.describe()","metadata":{"papermill":{"duration":0.177485,"end_time":"2023-09-13T18:55:38.029051","exception":false,"start_time":"2023-09-13T18:55:37.851566","status":"completed"},"tags":[],"id":"e4fa41f6","outputId":"7f686fc7-d4db-4693-b349-3044ed84e097","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are 541K transactions in this data set. The negative Quantity and UnitPrice values will need further analysis for understanding, as negative numbers for these are unusual.","metadata":{"id":"F2FmDlMb6nEV"}},{"cell_type":"code","source":"# Summary statistics for categorical variables\ndf.describe(include='object')","metadata":{"papermill":{"duration":0.843701,"end_time":"2023-09-13T18:55:38.93013","exception":false,"start_time":"2023-09-13T18:55:38.086429","status":"completed"},"scrolled":true,"tags":[],"id":"38d8e05d","outputId":"7dea6276-c2c5-4c40-da31-a2b4d9755a6c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Of the 541K transactions, there are 4,223 unique descriptions (or products) purchased from 38 countries, most frequently the United Kingdom.","metadata":{"papermill":{"duration":0.060269,"end_time":"2023-09-13T18:55:39.048706","exception":false,"start_time":"2023-09-13T18:55:38.988437","status":"completed"},"tags":[],"id":"c5e67213"}},{"cell_type":"markdown","source":"### Handling missing values","metadata":{"papermill":{"duration":0.056136,"end_time":"2023-09-13T18:55:39.509898","exception":false,"start_time":"2023-09-13T18:55:39.453762","status":"completed"},"tags":[],"id":"44be9a3d"}},{"cell_type":"code","source":"def perc_mv(x, y):\n    perc = y.isnull().sum() / len(x) * 100\n    return perc\n\nprint('Missing value ratios(%):\\nDescription: {}\\nCustomerID: {}'.format(perc_mv(df, df['Description']),\n                                                                                   perc_mv(df, df['CustomerID'])\n                                                                                   ))","metadata":{"id":"PugznmoC8WtU","outputId":"0cb3541f-6554-421b-f8a8-6a3df0eea9b1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe `CustomerID` column contains nearly a quarter of missing data. As clustering is based on customer behavior and preferences, it's crucial to have accurate data on customer identifiers. As such there is no viable Inputting strategy for this scenario. These rows will need to be dropped.\n    \nThe `Description` column has a minor percentage of missing values, and can be safely removed.","metadata":{"papermill":{"duration":0.0648,"end_time":"2023-09-13T18:55:40.684001","exception":false,"start_time":"2023-09-13T18:55:40.619201","status":"completed"},"tags":[],"id":"b59b382a"}},{"cell_type":"code","source":"# Extracting rows with missing values in 'CustomerID' or 'Description' columns\ndf[df['CustomerID'].isnull() | df['Description'].isnull()].head()","metadata":{"papermill":{"duration":0.161893,"end_time":"2023-09-13T18:55:40.910774","exception":false,"start_time":"2023-09-13T18:55:40.748881","status":"completed"},"tags":[],"id":"1ec499ed","outputId":"d4a7e4df-ccc6-4c56-8c6e-e056d0dec745","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"id":"2Uh3SjAz4895","outputId":"f7494860-2a8d-473b-d0a0-d687f4b1c0d3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Removing rows with missing values in 'CustomerID' and 'Description' columns\ndf = df.dropna(subset=['CustomerID', 'Description'])","metadata":{"papermill":{"duration":0.200287,"end_time":"2023-09-13T18:55:41.174169","exception":false,"start_time":"2023-09-13T18:55:40.973882","status":"completed"},"tags":[],"id":"9be2cf0c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verifying the removal of missing values\ndf.isnull().sum()","metadata":{"papermill":{"duration":0.283735,"end_time":"2023-09-13T18:55:41.5152","exception":false,"start_time":"2023-09-13T18:55:41.231465","status":"completed"},"tags":[],"id":"7c7a2e89","outputId":"97017e4b-7876-42f6-ee60-b054f3d15155","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"id":"tXbr1t-B46h0","outputId":"0272d027-3a51-4e93-9551-da0bbce69619","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checks for duplicates","metadata":{"papermill":{"duration":0.059306,"end_time":"2023-09-13T18:55:41.748028","exception":false,"start_time":"2023-09-13T18:55:41.688722","status":"completed"},"tags":[],"id":"d2c36b71"}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"id":"3FG2qqIK40P_","outputId":"44de2db2-a08e-4c13-fc47-74b468f648e3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop_duplicates(inplace=True)","metadata":{"papermill":{"duration":0.854262,"end_time":"2023-09-13T18:55:43.343966","exception":false,"start_time":"2023-09-13T18:55:42.489704","status":"completed"},"tags":[],"id":"82d2b098","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Getting the number of rows in the dataframe\ndf.shape","metadata":{"papermill":{"duration":0.073206,"end_time":"2023-09-13T18:55:43.478411","exception":false,"start_time":"2023-09-13T18:55:43.405205","status":"completed"},"tags":[],"id":"96102471","outputId":"d20485ee-3643-4fce-a5a1-8833b10943e6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checks for noise\nAnalysis of negative values in Quantity and UnitPrice columns","metadata":{"papermill":{"duration":0.060407,"end_time":"2023-09-13T18:55:43.599222","exception":false,"start_time":"2023-09-13T18:55:43.538815","status":"completed"},"tags":[],"id":"23ff4f89"}},{"cell_type":"code","source":"df[df['Quantity'] < 0].head(20)","metadata":{"id":"cd47wbIG89tg","outputId":"53681693-bc8f-4024-b01b-561fe081af14","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['Quantity'] < 0].shape","metadata":{"id":"EM7VSKvz9F7A","outputId":"3b6fbe28-5096-412c-bc3e-f52d7e921591","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are quite a lot of these transactions. If we had contact with the owners we would request additional information on these negative values to better understand the data. In the absence of that, some kagglers in the Discussion threads have suggested that the negative values represent returned or cancelled items, both of which seem feasible.","metadata":{"id":"OYmZjXcyHleC"}},{"cell_type":"code","source":"df[df['Quantity'] < 0]['InvoiceNo'].unique()","metadata":{"id":"6MLe1RBBIs-3","outputId":"6911e138-ccd8-459f-afd6-8cddfaa3092c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(df[df['Quantity'] < 0]['InvoiceNo'].head(1000).unique())","metadata":{"id":"V_1lqm-0JLLw","outputId":"ac3c9718-866b-43dd-95d2-fd254bab8f8d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Is it that all rows with negative quantity values have an Invoice number that starts with \"C\"?","metadata":{"id":"hHlSHZGLN5AT"}},{"cell_type":"code","source":"df[(df['InvoiceNo'].str[0] != 'C') & (df['Quantity'] < 0)]['InvoiceNo'].head()","metadata":{"id":"0jvGdAwgJzqf","outputId":"c069c888-9bd1-4313-f910-1f14c8a9006b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[(df['InvoiceNo'].str[0] == 'C') & (df['Quantity'] > 0)]['InvoiceNo'].head()","metadata":{"id":"TDSYr7IELSmu","outputId":"85246575-02f1-46ed-e68b-f727c0535491","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For all rows where Quantity is less than 0, the invoice number starts with \"C\". It seems these really are \"Cancelled\" transactions.","metadata":{"id":"Ws-1bP0_LZUb"}},{"cell_type":"code","source":"display(df[df['Quantity'] < 0]['StockCode'].head(1000).unique())","metadata":{"id":"zG8_TMWYL7f_","outputId":"c1e4f1dd-9e3c-4c35-d3de-e4ba2d851143","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most Stock Codes are 5 characters in length. Let's take a closer look at the exceptions.","metadata":{"id":"olqqNIaIXG0T"}},{"cell_type":"code","source":"df[(df['StockCode'].str.len() != 5) & (df['Quantity'] < 0)]['StockCode'].head(50)","metadata":{"id":"tLETh_TBMHQ0","outputId":"812fd789-5699-44a1-9d8c-6f0aef396988","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[(df['StockCode'].str.len() == 6) & (df['Quantity'] < 0)].sort_values(by='StockCode').head(50)","metadata":{"id":"QVMznI4GNAyN","outputId":"d58a5c3b-b501-4061-9dfa-c81656245de4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Stock Codes with the letters seem to be different variations of the same product. Let's verify this is the case with quantities more than 0.","metadata":{"id":"BtmeA_FDNRgs"}},{"cell_type":"code","source":"df[(df['StockCode'].str.len() == 6) & (df['Quantity'] > 0)].sort_values(by='StockCode').head(50)","metadata":{"id":"EJQbPxkVNj-z","outputId":"a0f3a9e7-642a-4a4b-fb05-80d59497b8c1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[(df['StockCode'].str.len() < 5) & (df['Quantity'] < 0)]['StockCode'].unique()","metadata":{"id":"fCufSZwsNtBW","outputId":"e791f5cc-09b8-45cb-d202-f7b3a498e79d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[(df['StockCode'].str.len() < 5) & (df['Quantity'] > 0)]['StockCode'].unique()","metadata":{"id":"58D4D_L9VBJB","outputId":"7f136c2c-6908-4618-efd9-819546dfbfbc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[(df['StockCode'].str.len() < 5)]['StockCode'].count()","metadata":{"id":"XD1mONsjVG3S","outputId":"4ac7d8a1-35d5-446f-de3c-3b01146c46be","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[(df['StockCode'].str.len() > 6)]['StockCode'].count()","metadata":{"id":"eLxyyVYsVMEL","outputId":"438f9169-1e63-4d56-b08c-ffa24eef2e2d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are a very small number of transactions with a Stock Code that is not 5 or 6 characters in length. Due to the small number these can safely be dropped from the data set.","metadata":{"id":"plpvdOebWENM"}},{"cell_type":"code","source":"to_remove = df[df['StockCode'].str.len() < 5]['StockCode'].unique()","metadata":{"id":"boVj0_KvauLa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"to_remove2 = df[df['StockCode'].str.len() > 6]['StockCode'].unique()","metadata":{"id":"yv38Rj8khFxy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"to_remove = np.concatenate((to_remove, to_remove2))\nto_remove = np.unique(to_remove)","metadata":{"id":"AIhIPlkZhM1J","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[~df['StockCode'].isin(to_remove)]","metadata":{"id":"1Bz-osj1hlUL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Getting the number of rows in the dataframe\ndf.shape[0]","metadata":{"papermill":{"duration":0.070041,"end_time":"2023-09-13T18:55:47.495154","exception":false,"start_time":"2023-09-13T18:55:47.425113","status":"completed"},"tags":[],"id":"bc3dda95","outputId":"b4657b51-92f1-4576-d18e-05a2aaeb7d7b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['UnitPrice'] < 0].head(20)","metadata":{"id":"S9PCeKyq9fTH","outputId":"165c71fe-c749-4f44-bde5-f4f4598dd05d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems this was removed when null values were handled.\n\nAs we have identified the rows with cancelled transaction, let us create a \"Transaction Status\" column with \"Completed\" and \"cancelled\"","metadata":{"id":"URt9Dy4191kO"}},{"cell_type":"code","source":"# Filter out the rows with InvoiceNo starting with \"C\" and create a new column indicating the transaction status\ndf['Transaction_Status'] = np.where(df['InvoiceNo'].astype(str).str.startswith('C'), 'Cancelled', 'Completed')\n\n# Analyze the characteristics of these rows (considering the new column)\ncancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']\ncancelled_transactions.describe().drop('CustomerID', axis=1)","metadata":{"papermill":{"duration":0.486159,"end_time":"2023-09-13T18:55:44.294362","exception":false,"start_time":"2023-09-13T18:55:43.808203","status":"completed"},"tags":[],"id":"19e11e75","outputId":"e2b2b095-a8c6-408b-dd44-c19db3bd4bbb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Let's investigate the transactions with 0 price","metadata":{"papermill":{"duration":0.064272,"end_time":"2023-09-13T18:55:50.169305","exception":false,"start_time":"2023-09-13T18:55:50.105033","status":"completed"},"tags":[],"id":"464073aa"}},{"cell_type":"code","source":"df['UnitPrice'].describe()","metadata":{"papermill":{"duration":0.097583,"end_time":"2023-09-13T18:55:50.456794","exception":false,"start_time":"2023-09-13T18:55:50.359211","status":"completed"},"scrolled":true,"tags":[],"id":"81d5299a","outputId":"81c8f0cd-d43b-406b-83ba-3f6436ca8622","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['UnitPrice']==0].describe()[['Quantity']]","metadata":{"papermill":{"duration":0.09384,"end_time":"2023-09-13T18:55:50.748583","exception":false,"start_time":"2023-09-13T18:55:50.654743","status":"completed"},"tags":[],"id":"c08c5b8b","outputId":"955b34a5-0127-4cdb-c3ee-ef1a646e8510","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These are either discounts/puchases with coupons or errors. Given the small number of these transactions (33), we can proceed to remove these transactions from the dataset.","metadata":{"papermill":{"duration":0.064739,"end_time":"2023-09-13T18:55:51.008917","exception":false,"start_time":"2023-09-13T18:55:50.944178","status":"completed"},"tags":[],"id":"cdc32fb9"}},{"cell_type":"code","source":"# Removing records with a unit price of zero to avoid potential data entry errors\ndf = df[df['UnitPrice'] > 0]","metadata":{"papermill":{"duration":0.125633,"end_time":"2023-09-13T18:55:51.201038","exception":false,"start_time":"2023-09-13T18:55:51.075405","status":"completed"},"tags":[],"id":"a8fd2d24","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standardize the text to uppercase to maintain uniformity across the dataset\ndf['Description'] = df['Description'].str.upper()","metadata":{"papermill":{"duration":0.14653,"end_time":"2023-09-13T18:55:49.044148","exception":false,"start_time":"2023-09-13T18:55:48.897618","status":"completed"},"tags":[],"id":"9aaa4a45","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the occurrence of each unique description and sort them\ndescription_counts = df['Description'].value_counts()\n\n# Get the top 30 descriptions\ntop_30_descriptions = description_counts[:30]\n\n# Plotting\nplt.figure(figsize=(8,6))\nplt.barh(top_30_descriptions.index[::-1], top_30_descriptions.values[::-1], color='#ff6200')\n\n# Adding labels and title\nplt.xlabel('Number of Occurrences')\nplt.ylabel('Description')\nplt.title('Top 30 Most Frequent Descriptions')\n\n# Show the plot\nplt.show()","metadata":{"papermill":{"duration":0.906535,"end_time":"2023-09-13T18:55:48.705153","exception":false,"start_time":"2023-09-13T18:55:47.798618","status":"completed"},"tags":[],"id":"ebd4eebb","outputId":"3ee6a13f-a390-4a2e-f77c-cbe5093b028c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resetting the index of the cleaned dataset\ndf.reset_index(drop=True, inplace=True)","metadata":{"papermill":{"duration":0.075851,"end_time":"2023-09-13T18:55:51.600414","exception":false,"start_time":"2023-09-13T18:55:51.524563","status":"completed"},"tags":[],"id":"8fb2e314","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Getting the number of rows in the dataframe\ndf.shape[0]","metadata":{"papermill":{"duration":0.077309,"end_time":"2023-09-13T18:55:51.743412","exception":false,"start_time":"2023-09-13T18:55:51.666103","status":"completed"},"tags":[],"id":"e53a70e2","outputId":"bfa4aede-858e-4a1f-b744-920825b98173","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering - RFM","metadata":{"papermill":{"duration":0.065855,"end_time":"2023-09-13T18:55:51.87548","exception":false,"start_time":"2023-09-13T18:55:51.809625","status":"completed"},"tags":[],"id":"c0423204"}},{"cell_type":"markdown","source":"Recency - most recent purchase per customer","metadata":{"papermill":{"duration":0.065054,"end_time":"2023-09-13T18:55:52.406366","exception":false,"start_time":"2023-09-13T18:55:52.341312","status":"completed"},"tags":[],"id":"49247c5e"}},{"cell_type":"code","source":"df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n\ndf['InvoiceDay'] = df['InvoiceDate'].dt.date\n\n# Find the most recent purchase date for each customer\ncustomer_data = df[df['Transaction_Status'] == 'Completed'].groupby('CustomerID')['InvoiceDay'].max().reset_index()\n\n# Find the most recent date in the entire dataset\nmost_recent_date = df[df['Transaction_Status'] == 'Completed']['InvoiceDay'].max()\n\n# Convert InvoiceDay to datetime type before subtraction\ncustomer_data['InvoiceDay'] = pd.to_datetime(customer_data['InvoiceDay'])\nmost_recent_date = pd.to_datetime(most_recent_date)\n\n# Calculate the number of days since the last purchase for each customer\ncustomer_data['Days_Since_Last_Purchase'] = (most_recent_date - customer_data['InvoiceDay']).dt.days\n\n# Remove the InvoiceDay column\ncustomer_data.drop(columns=['InvoiceDay'], inplace=True)\n\ncustomer_data.head()","metadata":{"papermill":{"duration":1.173973,"end_time":"2023-09-13T18:55:53.778716","exception":false,"start_time":"2023-09-13T18:55:52.604743","status":"completed"},"tags":[],"id":"12e426e9","outputId":"7f608cb2-15f9-4b7b-a21d-c28305002209","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Frequency","metadata":{"papermill":{"duration":0.063722,"end_time":"2023-09-13T18:55:54.31809","exception":false,"start_time":"2023-09-13T18:55:54.254368","status":"completed"},"tags":[],"id":"d3b5ec72"}},{"cell_type":"markdown","source":"Here we create two features that quantify the frequency of a customer's engagement with the retailer:\n\n- __Total Transactions__: The total number of transactions made by a customer.\n\n    \n\n- __Total Products Purchased__: The total number of products (sum of quantities) purchased by a customer across all transactions. This provides insight into the customer's buying behavior in terms of the volume of products purchased.","metadata":{"papermill":{"duration":0.065174,"end_time":"2023-09-13T18:55:54.450537","exception":false,"start_time":"2023-09-13T18:55:54.385363","status":"completed"},"tags":[],"id":"a5b9dce3"}},{"cell_type":"code","source":"# Calculate the total number of transactions made by each customer\ntotal_transactions = df[df['Transaction_Status'] == 'Completed'].groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\ntotal_transactions.rename(columns={'InvoiceNo': 'Total_Transactions'}, inplace=True)\n\n# Calculate the total number of products purchased by each customer\ntotal_products_purchased = df[df['Transaction_Status'] == 'Completed'].groupby('CustomerID')['Quantity'].sum().reset_index()\ntotal_products_purchased.rename(columns={'Quantity': 'Total_Products_Purchased'}, inplace=True)\n\n# Merge the new features into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, total_transactions, on='CustomerID')\ncustomer_data = pd.merge(customer_data, total_products_purchased, on='CustomerID')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":0.187211,"end_time":"2023-09-13T18:55:54.703665","exception":false,"start_time":"2023-09-13T18:55:54.516454","status":"completed"},"tags":[],"id":"49c8b928","outputId":"ae41de2e-ecef-4736-b787-7a33a8743755","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Monetary","metadata":{"papermill":{"duration":0.085369,"end_time":"2023-09-13T18:55:54.854478","exception":false,"start_time":"2023-09-13T18:55:54.769109","status":"completed"},"tags":[],"id":"6fa3261c"}},{"cell_type":"markdown","source":"Here, we create two features that represent the monetary aspect of customer's transactions:\n\n- __Total Spend__: Total amount of money spent per customer, calculated as the sum of the product of `UnitPrice` and `Quantity` for all transactions per customer.\n\n    \n- __Average Transaction Value__: __Total Spend__ divided by the __Total Transactions__ per customer. This indicates the average transaction value per customer. This metric is useful in understanding the spending behavior of customers per transaction.","metadata":{"papermill":{"duration":0.065187,"end_time":"2023-09-13T18:55:54.985249","exception":false,"start_time":"2023-09-13T18:55:54.920062","status":"completed"},"tags":[],"id":"ccd2f4f8"}},{"cell_type":"code","source":"# Calculate the total spend by each customer\ndf['Total_Spend'] = df['UnitPrice'] * df['Quantity']\ntotal_spend = df[df['Transaction_Status'] == 'Completed'].groupby('CustomerID')['Total_Spend'].sum().reset_index()\n\n# Calculate the average transaction value for each customer\naverage_transaction_value = total_spend.merge(total_transactions, on='CustomerID')\naverage_transaction_value['Average_Transaction_Value'] = average_transaction_value['Total_Spend'] / average_transaction_value['Total_Transactions']\n\n# Merge the new features into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, total_spend, on='CustomerID')\ncustomer_data = pd.merge(customer_data, average_transaction_value[['CustomerID', 'Average_Transaction_Value']], on='CustomerID')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":0.121753,"end_time":"2023-09-13T18:55:55.178675","exception":false,"start_time":"2023-09-13T18:55:55.056922","status":"completed"},"tags":[],"id":"a131e718","outputId":"f2d5bf92-c9ed-47f2-dda6-b6af90196dc5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Other useful features","metadata":{"papermill":{"duration":0.067521,"end_time":"2023-09-13T18:55:55.315371","exception":false,"start_time":"2023-09-13T18:55:55.24785","status":"completed"},"tags":[],"id":"48c28443"}},{"cell_type":"markdown","source":"__Unique Products Purchased__: This feature represents the number of distinct products bought by a customer. A higher value indicates that the customer has a diverse taste or preference, buying a wide range of products, while a lower value might indicate a focused or specific preference. Understanding the diversity in product purchases can help in segmenting customers based on their buying diversity, which can be a critical input in personalizing product recommendations.","metadata":{"papermill":{"duration":0.066321,"end_time":"2023-09-13T18:55:55.449792","exception":false,"start_time":"2023-09-13T18:55:55.383471","status":"completed"},"tags":[],"id":"e062ecf4"}},{"cell_type":"code","source":"# Calculate the number of unique products purchased by each customer\nunique_products_purchased = df[df['Transaction_Status'] == 'Completed'].groupby('CustomerID')['StockCode'].nunique().reset_index()\nunique_products_purchased.rename(columns={'StockCode': 'Unique_Products_Purchased'}, inplace=True)\n\n# Merge the new feature into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, unique_products_purchased, on='CustomerID')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":0.251802,"end_time":"2023-09-13T18:55:55.767945","exception":false,"start_time":"2023-09-13T18:55:55.516143","status":"completed"},"tags":[],"id":"055ab01f","outputId":"e6168590-499c-410d-9229-c22b39b78c81","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Average Days Between Purchases__: This feature represents the average number of days a customer waits before making another purchase. Understanding this can help in predicting when the customer is likely to make their next purchase, which can be a crucial metric for targeted marketing and personalized promotions.\n\n    \n__Favorite Shopping Day__: This denotes the day of the week when the customer shops the most. This information can help in identifying the preferred shopping days of different customer segments, which can be used to optimize marketing strategies and promotions for different days of the week.\n\n    \n__Favorite Shopping Hour__: This refers to the hour of the day when the customer shops the most. Identifying the favorite shopping hour can aid in optimizing the timing of marketing campaigns and promotions to align with the times when different customer segments are most active.\n\n    \nBy including these behavioral features in our dataset, we can create a more rounded view of our customers, which will potentially enhance the effectiveness of the clustering algorithm, leading to more meaningful customer segments.","metadata":{"papermill":{"duration":0.068002,"end_time":"2023-09-13T18:55:56.037118","exception":false,"start_time":"2023-09-13T18:55:55.969116","status":"completed"},"tags":[],"id":"115bd31a"}},{"cell_type":"code","source":"# Extract day of week and hour from InvoiceDate\ndf['Day_Of_Week'] = df['InvoiceDate'].dt.dayofweek\ndf['Hour'] = df['InvoiceDate'].dt.hour\n\n# Calculate the average number of days between consecutive purchases\ndays_between_purchases = df[df['Transaction_Status'] == 'Completed'].groupby('CustomerID')['InvoiceDay'].apply(lambda x: (x.diff().dropna()).apply(lambda y: y.days))\naverage_days_between_purchases = days_between_purchases.groupby('CustomerID').mean().reset_index()\naverage_days_between_purchases.rename(columns={'InvoiceDay': 'Average_Days_Between_Purchases'}, inplace=True)\n\n# Find the favorite shopping day of the week\nfavorite_shopping_day = df[df['Transaction_Status'] == 'Completed'].groupby(['CustomerID', 'Day_Of_Week']).size().reset_index(name='Count')\nfavorite_shopping_day = favorite_shopping_day.loc[favorite_shopping_day.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Day_Of_Week']]\n\n# Find the favorite shopping hour of the day\nfavorite_shopping_hour = df[df['Transaction_Status'] == 'Completed'].groupby(['CustomerID', 'Hour']).size().reset_index(name='Count')\nfavorite_shopping_hour = favorite_shopping_hour.loc[favorite_shopping_hour.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Hour']]\n\n# Merge the new features into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, average_days_between_purchases, on='CustomerID')\ncustomer_data = pd.merge(customer_data, favorite_shopping_day, on='CustomerID')\ncustomer_data = pd.merge(customer_data, favorite_shopping_hour, on='CustomerID')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":7.324267,"end_time":"2023-09-13T18:56:03.428738","exception":false,"start_time":"2023-09-13T18:55:56.104471","status":"completed"},"tags":[],"id":"25325c84","outputId":"a50a7cb3-6360-420a-f57a-60c920c41a14","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Country__: This feature identifies the country where each customer is located. Different regions might have varying preferences and purchasing behaviors which can be critical in personalizing marketing strategies and inventory planning.","metadata":{"papermill":{"duration":0.08516,"end_time":"2023-09-13T18:56:03.724226","exception":false,"start_time":"2023-09-13T18:56:03.639066","status":"completed"},"tags":[],"id":"869bd60e"}},{"cell_type":"code","source":"df['Country'].value_counts(normalize=True).head()","metadata":{"papermill":{"duration":0.150849,"end_time":"2023-09-13T18:56:03.944819","exception":false,"start_time":"2023-09-13T18:56:03.79397","status":"completed"},"tags":[],"id":"517c5883","outputId":"457111d4-9827-41f9-87a3-374e5ed86b60","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Given that a substantial portion (__89%__) of transactions originate from the __United Kingdom__, we can create a binary feature indicating whether the transaction is from the UK or not. This approach can potentially streamline the clustering process without losing critical geographical information.","metadata":{"papermill":{"duration":0.066789,"end_time":"2023-09-13T18:56:04.078489","exception":false,"start_time":"2023-09-13T18:56:04.0117","status":"completed"},"tags":[],"id":"e338f422"}},{"cell_type":"code","source":"# Group by CustomerID and Country to get the number of transactions per country for each customer\ncustomer_country = df[df['Transaction_Status'] == 'Completed'].groupby(['CustomerID', 'Country']).size().reset_index(name='Number_of_Transactions')\n\n# Get the country with the maximum number of transactions for each customer (in case a customer has transactions from multiple countries)\ncustomer_main_country = customer_country.sort_values('Number_of_Transactions', ascending=False).drop_duplicates('CustomerID')\n\n# Create a binary column indicating whether the customer is from the UK or not\ncustomer_main_country['Is_UK'] = np.where(customer_main_country['Country'] == 'United Kingdom', 1, 0)\n\n# Merge this data with our customer_data dataframe\ncustomer_data = pd.merge(customer_data, customer_main_country[['CustomerID', 'Is_UK']], on='CustomerID', how='left')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":0.167104,"end_time":"2023-09-13T18:56:04.448331","exception":false,"start_time":"2023-09-13T18:56:04.281227","status":"completed"},"tags":[],"id":"076d45c4","outputId":"c43d0f9e-eb68-47c2-ff49-514611d6e006","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display feature distribution\ncustomer_data['Is_UK'].value_counts()","metadata":{"papermill":{"duration":0.081428,"end_time":"2023-09-13T18:56:04.601164","exception":false,"start_time":"2023-09-13T18:56:04.519736","status":"completed"},"tags":[],"id":"172aabff","outputId":"a2ed2c34-8bce-43f0-9d83-629ef1cf019d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cancellations","metadata":{"papermill":{"duration":0.068165,"end_time":"2023-09-13T18:56:04.7361","exception":false,"start_time":"2023-09-13T18:56:04.667935","status":"completed"},"tags":[],"id":"54878de8"}},{"cell_type":"markdown","source":"__Cancellation Frequency__: Total number of transactions a customer has canceled. Understanding the frequency of cancellations can help us identify customers who are more likely to cancel transactions.\n\n    \n__Cancellation Rate__: Proportion of transactions that a customer has canceled out of all their transactions. This metric gives a normalized view of cancellation behavior.","metadata":{"papermill":{"duration":0.068456,"end_time":"2023-09-13T18:56:04.871071","exception":false,"start_time":"2023-09-13T18:56:04.802615","status":"completed"},"tags":[],"id":"c221e4d1"}},{"cell_type":"code","source":"# Calculate the total number of transactions made by each customer\ntotal_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n\n# Calculate the number of cancelled transactions for each customer\ncancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']\ncancellation_frequency = cancelled_transactions.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\ncancellation_frequency.rename(columns={'InvoiceNo': 'Cancellation_Frequency'}, inplace=True)\n\n# Merge the Cancellation Frequency data into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, cancellation_frequency, on='CustomerID', how='left')\n\n# Replace NaN values with 0 (for customers who have not cancelled any transaction)\ncustomer_data['Cancellation_Frequency'].fillna(0, inplace=True)\n\n# Calculate the Cancellation Rate\ncustomer_data['Cancellation_Rate'] = customer_data['Cancellation_Frequency'] / total_transactions['InvoiceNo']\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":0.264581,"end_time":"2023-09-13T18:56:05.20251","exception":false,"start_time":"2023-09-13T18:56:04.937929","status":"completed"},"tags":[],"id":"65a2c273","outputId":"e76c6893-a899-43bb-bd40-0aa145955f49","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"__Monthly_Spending_Mean__: Average monthly spend per customer.","metadata":{"papermill":{"duration":0.065548,"end_time":"2023-09-13T18:56:05.465159","exception":false,"start_time":"2023-09-13T18:56:05.399611","status":"completed"},"tags":[],"id":"597477c3"}},{"cell_type":"code","source":"# Extract month and year from InvoiceDate\ndf['Year'] = df['InvoiceDate'].dt.year\ndf['Month'] = df['InvoiceDate'].dt.month\n\n# Calculate monthly spending for each customer\nmonthly_spending = df.groupby(['CustomerID', 'Year', 'Month'])['Total_Spend'].sum().reset_index()\n\n# Calculate Seasonal Buying Patterns: We are using monthly frequency as a proxy for seasonal buying patterns\nseasonal_buying_patterns = monthly_spending.groupby('CustomerID')['Total_Spend'].agg(['mean']).reset_index()\nseasonal_buying_patterns.rename(columns={'mean': 'Monthly_Spending_Mean'}, inplace=True)\n\n# Replace NaN values in Monthly_Spending_Std with 0, implying no variability for customers with single transaction month\n\n# Merge the new features into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, seasonal_buying_patterns, on='CustomerID')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":0.835651,"end_time":"2023-09-13T18:56:06.367576","exception":false,"start_time":"2023-09-13T18:56:05.531925","status":"completed"},"tags":[],"id":"c5720ea3","outputId":"441c298d-562d-4ab9-fa86-f14ee05e92c3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing the data type of 'CustomerID' to string as it is a unique identifier and not used in mathematical operations\ncustomer_data['CustomerID'] = customer_data['CustomerID'].astype(str)\n\n# Convert data types of columns to optimal types\ncustomer_data = customer_data.convert_dtypes()","metadata":{"papermill":{"duration":0.103278,"end_time":"2023-09-13T18:56:06.675301","exception":false,"start_time":"2023-09-13T18:56:06.572023","status":"completed"},"tags":[],"id":"ced9cc9e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"customer_data.head(10)","metadata":{"papermill":{"duration":0.102034,"end_time":"2023-09-13T18:56:06.846461","exception":false,"start_time":"2023-09-13T18:56:06.744427","status":"completed"},"tags":[],"id":"ad4a945c","outputId":"cc34e096-be2d-4c3e-ebe7-8bffef723ad1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"customer_data.info()","metadata":{"papermill":{"duration":0.09233,"end_time":"2023-09-13T18:56:07.006785","exception":false,"start_time":"2023-09-13T18:56:06.914455","status":"completed"},"tags":[],"id":"84886ab4","outputId":"308458d5-0222-473f-839d-116cb994ef2b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Outlier Detection and Treatment","metadata":{"papermill":{"duration":0.069589,"end_time":"2023-09-13T18:56:07.701447","exception":false,"start_time":"2023-09-13T18:56:07.631858","status":"completed"},"tags":[],"id":"850905bd"}},{"cell_type":"code","source":"# prompt: do boxplots of customer_data\n\ncustomer_data.boxplot(figsize=(16, 10))\nplt.xticks(rotation=90)\nplt.show()\n","metadata":{"id":"0C1lv6Of03Jv","outputId":"b8c6802d-9260-48e7-9a89-95233110f994","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Given the multi-dimensional nature of the data, we will use the __Isolation Forest__ algorithm that can detect outliers in multi-dimensional spaces.","metadata":{"papermill":{"duration":0.069178,"end_time":"2023-09-13T18:56:07.840934","exception":false,"start_time":"2023-09-13T18:56:07.771756","status":"completed"},"tags":[],"id":"028cb74d"}},{"cell_type":"code","source":"# Initializing the IsolationForest model with a contamination parameter of 0.05\nmodel = IsolationForest(contamination=0.05, random_state=0)\n\n# Fitting the model on our dataset (converting DataFrame to NumPy to avoid warning)\ncustomer_data['Outlier_Scores'] = model.fit_predict(customer_data.iloc[:, 1:].to_numpy())\n\n# Creating a new column to identify outliers (1 for inliers and -1 for outliers)\ncustomer_data['Is_Outlier'] = [1 if x == -1 else 0 for x in customer_data['Outlier_Scores']]\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"papermill":{"duration":0.728447,"end_time":"2023-09-13T18:56:08.63979","exception":false,"start_time":"2023-09-13T18:56:07.911343","status":"completed"},"tags":[],"id":"a067ae0a","outputId":"603f1a2a-d2a5-469f-96c8-dec72c2991d5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n    \nAfter applying the Isolation Forest algorithm, we have identified the outliers and marked them in a new column named `Is_Outlier`. We have also calculated the outlier scores which represent the anomaly score of each record.\n\nNow let's visualize the distribution of these scores and the number of inliers and outliers detected by the model:","metadata":{"papermill":{"duration":0.070961,"end_time":"2023-09-13T18:56:08.782642","exception":false,"start_time":"2023-09-13T18:56:08.711681","status":"completed"},"tags":[],"id":"16a1388e"}},{"cell_type":"code","source":"# Calculate the percentage of inliers and outliers\noutlier_percentage = customer_data['Is_Outlier'].value_counts(normalize=True) * 100\n\n# Plotting the percentage of inliers and outliers\nplt.figure(figsize=(12, 4))\noutlier_percentage.plot(kind='barh', color='#ff6200')\n\n# Adding the percentage labels on the bars\nfor index, value in enumerate(outlier_percentage):\n    plt.text(value, index, f'{value:.2f}%', fontsize=15)\n\nplt.title('Percentage of Inliers and Outliers')\nplt.xticks(ticks=np.arange(0, 115, 5))\nplt.xlabel('Percentage (%)')\nplt.ylabel('Is Outlier')\nplt.gca().invert_yaxis()\nplt.show()","metadata":{"papermill":{"duration":0.637863,"end_time":"2023-09-13T18:56:09.494637","exception":false,"start_time":"2023-09-13T18:56:08.856774","status":"completed"},"tags":[],"id":"2a156aa4","outputId":"38212d52-c0b6-4d75-fd6f-8076640f633b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the above plot, we can observe that about 5% of the customers have been identified as outliers in our dataset. We now separate the outliers from the main dataset.","metadata":{"papermill":{"duration":0.071032,"end_time":"2023-09-13T18:56:09.63536","exception":false,"start_time":"2023-09-13T18:56:09.564328","status":"completed"},"tags":[],"id":"0e41f9f5"}},{"cell_type":"code","source":"# Separate the outliers for analysis\noutliers_data = customer_data[customer_data['Is_Outlier'] == 1]\n\n# Remove the outliers from the main dataset\ncustomer_data_cleaned = customer_data[customer_data['Is_Outlier'] == 0]\n\n# Drop the 'Outlier_Scores' and 'Is_Outlier' columns\ncustomer_data_cleaned = customer_data_cleaned.drop(columns=['Outlier_Scores', 'Is_Outlier'])\n\n# Reset the index of the cleaned data\ncustomer_data_cleaned.reset_index(drop=True, inplace=True)","metadata":{"papermill":{"duration":0.095994,"end_time":"2023-09-13T18:56:09.94579","exception":false,"start_time":"2023-09-13T18:56:09.849796","status":"completed"},"tags":[],"id":"4662a17f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompt: do boxplots of customer_data\n\ncustomer_data_cleaned.boxplot(figsize=(16, 10))\nplt.xticks(rotation=90)\nplt.show()\n","metadata":{"id":"Wr7A-BLW4taG","outputId":"93c9e915-aabd-45fb-d499-20c4774ebec3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are still some outliers observed as per the boxplots, however the most extreme outliers were identified and removed.","metadata":{"id":"QLVyBQ5v5YIz"}},{"cell_type":"code","source":"# Getting the number of rows in the cleaned customer dataset\ncustomer_data_cleaned.shape[0]","metadata":{"papermill":{"duration":0.085015,"end_time":"2023-09-13T18:56:10.246151","exception":false,"start_time":"2023-09-13T18:56:10.161136","status":"completed"},"tags":[],"id":"22f3e398","outputId":"a9e5206b-d609-477d-96d1-0cb5cfb15940","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Correlation Analysis\n","metadata":{"papermill":{"duration":0.07047,"end_time":"2023-09-13T18:56:10.388162","exception":false,"start_time":"2023-09-13T18:56:10.317692","status":"completed"},"tags":[],"id":"c8510b52"}},{"cell_type":"code","source":"# Find columns with correlation of 0.7 or higher\ncorrelation_matrix = customer_data_cleaned.drop(columns=['CustomerID']).corr()\nhigh_correlation_cols_pos = correlation_matrix[correlation_matrix >= 0.7].stack().drop_duplicates().reset_index()\nhigh_correlation_cols_pos = high_correlation_cols_pos[high_correlation_cols_pos['level_0'] != high_correlation_cols_pos['level_1']]\n# Print the columns with high correlation\nprint(high_correlation_cols_pos)\n\nprint(\"\\n\")\n\nhigh_correlation_cols_neg = correlation_matrix[correlation_matrix <= -0.7].stack().drop_duplicates().reset_index()\nhigh_correlation_cols_neg = high_correlation_cols_neg[high_correlation_cols_neg['level_0'] != high_correlation_cols_neg['level_1']]\n# Print the columns with high correlation\nprint(high_correlation_cols_neg)","metadata":{"id":"r4IRgOuH5lWJ","outputId":"023ef616-fa0c-4a46-b6b2-5ad12f78d736","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Variables with high correlations that indicate multicollinearity:\n\n- `Monthly_Spending_Mean` and `Average_Transaction_Value`\n    \n    \n- `Total_Spend` and `Total_Products_Purchased`\n\n    \n- `Total_Transactions` and `Total_Spend`\n    \n    \n- `Cancellation_Rate` and `Cancellation_Frequency`\n    \n    \n- `Total_Transactions` and `Total_Products_Purchased`","metadata":{"papermill":{"duration":0.078511,"end_time":"2023-09-13T18:56:12.023961","exception":false,"start_time":"2023-09-13T18:56:11.94545","status":"completed"},"tags":[],"id":"c8b5e1c8"}},{"cell_type":"markdown","source":"We can consider treating the multicollinearity through dimensionality reduction techniques such as PCA to create a set of uncorrelated variables.","metadata":{"papermill":{"duration":0.079063,"end_time":"2023-09-13T18:56:12.185719","exception":false,"start_time":"2023-09-13T18:56:12.106656","status":"completed"},"tags":[],"id":"11588f38"}},{"cell_type":"markdown","source":"### Feature Scaling & Dimensionality Reduction","metadata":{"papermill":{"duration":0.078379,"end_time":"2023-09-13T18:56:12.342655","exception":false,"start_time":"2023-09-13T18:56:12.264276","status":"completed"},"tags":[],"id":"8a07521c"}},{"cell_type":"code","source":"# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# List of columns that don't need to be scaled\ncolumns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week']\n\n# List of columns that need to be scaled\ncolumns_to_scale = customer_data_cleaned.columns.difference(columns_to_exclude)\n\n# Copy the cleaned dataset\ncustomer_data_scaled = customer_data_cleaned.copy()\n\n# Applying the scaler to the necessary columns in the dataset\ncustomer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])\n\n# Display the first few rows of the scaled data\ncustomer_data_scaled.head()","metadata":{"papermill":{"duration":0.130165,"end_time":"2023-09-13T18:56:12.878414","exception":false,"start_time":"2023-09-13T18:56:12.748249","status":"completed"},"tags":[],"id":"6a2707ef","outputId":"39a4ec14-9b69-4815-bd64-ed4a2d5589b6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Given the multicollinearity we identified in our dataset, we will use PCA for dimensionality reduction as it works well in capturing linear relationships in the data.","metadata":{"papermill":{"duration":0.075645,"end_time":"2023-09-13T18:56:13.347039","exception":false,"start_time":"2023-09-13T18:56:13.271394","status":"completed"},"tags":[],"id":"2464b601"}},{"cell_type":"code","source":"# Setting CustomerID as the index column\ncustomer_data_scaled.set_index('CustomerID', inplace=True)\n\n# Apply PCA\npca = PCA().fit(customer_data_scaled)\n\n# Calculate the Cumulative Sum of the Explained Variance\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n# Set the optimal k value (for now we start with 6)\noptimal_k = 6\n\n# Set seaborn plot style\nsns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')\n\n# Plot the cumulative explained variance against the number of components\nplt.figure(figsize=(20, 10))\n\n# Bar chart for the explained variance of each component\nbarplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n                      y=explained_variance_ratio,\n                      color='#fcc36d',\n                      alpha=0.8)\n\n# Line plot for the cumulative explained variance\nlineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n                     marker='o', linestyle='--', color='#ff6200', linewidth=2)\n\n# Plot optimal k value line\noptimal_k_line = plt.axvline(optimal_k - 1, color='red', linestyle='--', label=f'Optimal k value = {optimal_k}')\n\n# Set labels and title\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Explained Variance', fontsize=14)\nplt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n\n# Customize ticks and legend\nplt.xticks(range(0, len(cumulative_explained_variance)))\nplt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n           loc=(0.62, 0.1),\n           frameon=True,\n           framealpha=1.0,\n           edgecolor='#ff6200')\n\n# Display the variance values for both graphs on the plots\nx_offset = -0.3\ny_offset = 0.01\nfor i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n    if i > 0:\n        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n\nplt.grid(axis='both')\nplt.show()","metadata":{"papermill":{"duration":1.164794,"end_time":"2023-09-13T18:56:14.748669","exception":false,"start_time":"2023-09-13T18:56:13.583875","status":"completed"},"tags":[],"id":"4c24948d","outputId":"8e431e23-48c1-43ec-fd4c-f14014708328","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we can observe that:\n\n- The first component explains approximately 29% of the variance.\n\n- The first two components together explain about 53% of the variance.\n\n- The first three components explain approximately 65% of the variance, and so on.\n\nFrom the plot, we can see that the increase in cumulative variance starts to slow down after the __5th component__ (which __captures about 81% of the total variance__). Retaining __the first 5 components__ could be a balanced choice, as they together explain a substantial portion of the total variance while reducing the dimensionality of the dataset.","metadata":{"papermill":{"duration":0.08209,"end_time":"2023-09-13T18:56:14.914553","exception":false,"start_time":"2023-09-13T18:56:14.832463","status":"completed"},"tags":[],"id":"24874c5b"}},{"cell_type":"code","source":"# Creating a PCA object with 5 components\npca = PCA(n_components=5)\n\n# Fitting and transforming the original data to the new PCA dataframe\ncustomer_data_pca = pca.fit_transform(customer_data_scaled)\n\n# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\ncustomer_data_pca = pd.DataFrame(customer_data_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n\n# Adding the CustomerID index back to the new PCA dataframe\ncustomer_data_pca.index = customer_data_scaled.index","metadata":{"papermill":{"duration":0.134162,"end_time":"2023-09-13T18:56:15.130666","exception":false,"start_time":"2023-09-13T18:56:14.996504","status":"completed"},"tags":[],"id":"b6a40979","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying the resulting dataframe based on the PCs\ncustomer_data_pca.head()","metadata":{"papermill":{"duration":0.103676,"end_time":"2023-09-13T18:56:15.396556","exception":false,"start_time":"2023-09-13T18:56:15.29288","status":"completed"},"tags":[],"id":"92d8befe","outputId":"abca175c-d3ce-4dea-edbe-0c41ab1ef491","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## K-Means Clustering","metadata":{"papermill":{"duration":0.085571,"end_time":"2023-09-13T18:56:16.038485","exception":false,"start_time":"2023-09-13T18:56:15.952914","status":"completed"},"tags":[],"id":"6eef0ce8"}},{"cell_type":"code","source":"# Set plot style, and background color\nsns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})\n\n# Set the color palette for the plot\nsns.set_palette(['#ff6200'])\n\n# Instantiate the clustering model with the specified parameters\nkm = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)\n\n# Create a figure and axis with the desired size\nfig, ax = plt.subplots(figsize=(12, 5))\n\n# Instantiate the KElbowVisualizer with the model and range of k values, and disable the timing plot\nvisualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)\n\n# Fit the data to the visualizer\nvisualizer.fit(customer_data_pca)\n\n# Finalize and render the figure\nvisualizer.show();","metadata":{"papermill":{"duration":18.47856,"end_time":"2023-09-13T18:56:35.752589","exception":false,"start_time":"2023-09-13T18:56:17.274029","status":"completed"},"tags":[],"id":"0f4f9237","outputId":"c1860149-f25d-420b-b78a-26a3363e8d4b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Using the YellowBrick library for the Elbow method, we observe that the suggested optimal k value is __5__. However, __we don't have a very distinct elbow point in this case__, and it seems that the inertia continues to decrease significantly up to k=5, indicating that __the optimum value of k could be between 3 and 7__. To choose the best k within this range, we employ the __silhouette analysis__.","metadata":{"papermill":{"duration":0.086402,"end_time":"2023-09-13T18:56:35.922228","exception":false,"start_time":"2023-09-13T18:56:35.835826","status":"completed"},"tags":[],"id":"225fd1c7"}},{"cell_type":"code","source":"def silhouette_analysis(df, start_k, stop_k, figsize=(15, 16)):\n\n    # Set the size of the figure\n    plt.figure(figsize=figsize)\n\n    # Create a grid with (stop_k - start_k + 1) rows and 2 columns\n    grid = gridspec.GridSpec(stop_k - start_k + 1, 2)\n\n    # Assign the first plot to the first row and both columns\n    first_plot = plt.subplot(grid[0, :])\n\n    # First plot: Silhouette scores for different k values\n    sns.set_palette(['darkorange'])\n\n    silhouette_scores = []\n\n    # Iterate through the range of k values\n    for k in range(start_k, stop_k + 1):\n        km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=0)\n        km.fit(df)\n        labels = km.predict(df)\n        score = silhouette_score(df, labels)\n        silhouette_scores.append(score)\n\n    best_k = start_k + silhouette_scores.index(max(silhouette_scores))\n\n    plt.plot(range(start_k, stop_k + 1), silhouette_scores, marker='o')\n    plt.xticks(range(start_k, stop_k + 1))\n    plt.xlabel('Number of clusters (k)')\n    plt.ylabel('Silhouette score')\n    plt.title('Average Silhouette Score for Different k Values', fontsize=15)\n\n\n    # Add the optimal k value text to the plot\n    optimal_k_text = f'The k value with the highest Silhouette score is: {best_k}'\n    plt.text(10, 0.23, optimal_k_text, fontsize=12, verticalalignment='bottom',\n             horizontalalignment='left', bbox=dict(facecolor='#fcc36d', edgecolor='#ff6200', boxstyle='round, pad=0.5'))\n\n\n    # Second plot (subplot): Silhouette plots for each k value\n    colors = sns.color_palette(\"bright\")\n\n    for i in range(start_k, stop_k + 1):\n        km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=0)\n        row_idx, col_idx = divmod(i - start_k, 2)\n\n        # Assign the plots to the second, third, and fourth rows\n        ax = plt.subplot(grid[row_idx + 1, col_idx])\n\n        visualizer = SilhouetteVisualizer(km, colors=colors, ax=ax)\n        visualizer.fit(df)\n\n        # Add the Silhouette score text to the plot\n        score = silhouette_score(df, km.labels_)\n        ax.text(0.97, 0.02, f'Silhouette Score: {score:.2f}', fontsize=12, \\\n                ha='right', transform=ax.transAxes, color='red')\n\n        ax.set_title(f'Silhouette Plot for {i} Clusters', fontsize=15)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"papermill":{"duration":0.105705,"end_time":"2023-09-13T18:56:36.609774","exception":false,"start_time":"2023-09-13T18:56:36.504069","status":"completed"},"tags":[],"id":"24135687","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"silhouette_analysis(customer_data_pca, 3, 12, figsize=(20, 50))","metadata":{"papermill":{"duration":48.801465,"end_time":"2023-09-13T18:57:25.496889","exception":false,"start_time":"2023-09-13T18:56:36.695424","status":"completed"},"tags":[],"id":"c971cf0b","outputId":"028d7fce-ce8a-4650-ea17-c6af5482792d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"k values of 3 and 4 are very close in silhouette scores. Based on the silhouette plots, k value of 3 seems to be the better choice.\n\nWe did the K-Means clustering for both k=3 and k=4, and found that the vizualization and scores(Silhouette Score, Calinski Harabasz Score and\nDavies Bouldin Score) were better with value of k=3. In this notebook we only show the K-Means with k=3.","metadata":{"papermill":{"duration":0.094984,"end_time":"2023-09-13T18:57:25.877053","exception":false,"start_time":"2023-09-13T18:57:25.782069","status":"completed"},"tags":[],"id":"1b72a390"}},{"cell_type":"code","source":"# Apply KMeans clustering using the optimal k\nkmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\nkmeans.fit(customer_data_pca)\ncustomer_data_cleaned['cluster'] = kmeans.labels_\ncustomer_data_pca['cluster'] = kmeans.labels_","metadata":{"papermill":{"duration":1.408565,"end_time":"2023-09-13T18:57:27.752578","exception":false,"start_time":"2023-09-13T18:57:26.344013","status":"completed"},"tags":[],"id":"d9c3238b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the first few rows of the original dataframe\ncustomer_data_cleaned.head()","metadata":{"papermill":{"duration":0.12762,"end_time":"2023-09-13T18:57:27.97386","exception":false,"start_time":"2023-09-13T18:57:27.84624","status":"completed"},"tags":[],"id":"b4ac520a","outputId":"2062b23b-fd78-4b19-a9a8-404c4b06e63b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute number of customers\nnum_observations = len(customer_data_pca)\n\n# Separate the features and the cluster labels\nX = customer_data_pca.drop('cluster', axis=1)\nclusters = customer_data_pca['cluster']\n\n# Compute the metrics\nsil_score = silhouette_score(X, clusters)\ncalinski_score = calinski_harabasz_score(X, clusters)\ndavies_score = davies_bouldin_score(X, clusters)\n\n# Create a table to display the metrics and the number of observations\ntable_data = [\n    [\"Number of Observations\", num_observations],\n    [\"Silhouette Score\", sil_score],\n    [\"Calinski Harabasz Score\", calinski_score],\n    [\"Davies Bouldin Score\", davies_score]\n]\n\n# Print the table\nprint(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))","metadata":{"papermill":{"duration":0.555512,"end_time":"2023-09-13T18:57:32.585864","exception":false,"start_time":"2023-09-13T18:57:32.030352","status":"completed"},"tags":[],"id":"6f85d0d4","outputId":"1ceacb32-82ca-46d0-f058-458709377e47","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"colors = sns.color_palette(\"bright\")\n\n# Calculate the percentage of customers in each cluster\ncluster_percentage = (customer_data_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\ncluster_percentage.columns = ['Cluster', 'Percentage']\ncluster_percentage.sort_values(by='Cluster', inplace=True)\n\n# Create a horizontal bar plot\nplt.figure(figsize=(10, 4))\nsns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n\n# Adding percentages on the bars\nfor index, value in enumerate(cluster_percentage['Percentage']):\n    plt.text(value+0.5, index, f'{value:.2f}%')\n\nplt.title('Distribution of Customers Across Clusters', fontsize=14)\nplt.xticks(ticks=np.arange(0, 50, 5))\nplt.xlabel('Percentage (%)')\n\n# Show the plot\nplt.show()","metadata":{"papermill":{"duration":0.55019,"end_time":"2023-09-13T18:57:30.939968","exception":false,"start_time":"2023-09-13T18:57:30.389778","status":"completed"},"tags":[],"id":"9c617e70","outputId":"c1558e37-9ade-448e-ffb0-8fd94e0b99e6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will use the top 3 PCAs (which capture the most variance in the data) to create a 3D visualization.","metadata":{"papermill":{"duration":0.096205,"end_time":"2023-09-13T18:57:28.738642","exception":false,"start_time":"2023-09-13T18:57:28.642437","status":"completed"},"tags":[],"id":"255f64c4"}},{"cell_type":"code","source":"# Setting up the color scheme for the clusters (RGB order)\ncolors = ['#e8000b', '#1ac938', '#023eff']","metadata":{"papermill":{"duration":0.104131,"end_time":"2023-09-13T18:57:28.937718","exception":false,"start_time":"2023-09-13T18:57:28.833587","status":"completed"},"tags":[],"id":"a9423286","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create separate data frames for each cluster\ncluster_0 = customer_data_pca[customer_data_pca['cluster'] == 0]\ncluster_1 = customer_data_pca[customer_data_pca['cluster'] == 1]\ncluster_2 = customer_data_pca[customer_data_pca['cluster'] == 2]\n\n# Create a 3D scatter plot\nfig = go.Figure()\n\n# Add data points for each cluster separately and specify the color\nfig.add_trace(go.Scatter3d(x=cluster_0['PC1'], y=cluster_0['PC2'], z=cluster_0['PC3'],\n                           mode='markers', marker=dict(color=colors[0], size=5, opacity=0.4), name='Cluster 0'))\nfig.add_trace(go.Scatter3d(x=cluster_1['PC1'], y=cluster_1['PC2'], z=cluster_1['PC3'],\n                           mode='markers', marker=dict(color=colors[1], size=5, opacity=0.4), name='Cluster 1'))\nfig.add_trace(go.Scatter3d(x=cluster_2['PC1'], y=cluster_2['PC2'], z=cluster_2['PC3'],\n                           mode='markers', marker=dict(color=colors[2], size=5, opacity=0.4), name='Cluster 2'))\n\n# Set the title and layout details\nfig.update_layout(\n    title=dict(text='3D Visualization of Customer Clusters in PCA Space', x=0.5),\n    scene=dict(\n        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC1'),\n        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC2'),\n        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC3'),\n    ),\n    width=900,\n    height=800\n)\n\n# Show the plot\nfig.show()","metadata":{"papermill":{"duration":0.482553,"end_time":"2023-09-13T18:57:29.514649","exception":false,"start_time":"2023-09-13T18:57:29.032096","status":"completed"},"tags":[],"id":"9f33fd0a","outputId":"bdeb39e9-ce40-418a-d561-38f55e35d736","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'customer_data_pca' is your DataFrame with columns 'PC1' to 'PC3', 'cluster'\n\n# Selecting the first six principal components\npcs = ['PC1', 'PC2', 'PC3']\n\n# Create a pair grid of scatterplots\nsns.set(style=\"ticks\")\nsns.pairplot(customer_data_pca, vars=pcs, hue='cluster', palette='viridis', diag_kind='kde', markers='o')\nplt.suptitle('Pairwise Scatterplots for Principal Components with Cluster Colors')\nplt.show()\n","metadata":{"id":"67aFyuuJU2rN","outputId":"811892d0-7eae-47a9-fd3b-7e6bf43b1e8e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization\n\nWe will visualize the clusters in a variety of ways:\n- separate radar plots\n- integrated radar plot\n- line plots","metadata":{"id":"Z10vRC7WZteW"}},{"cell_type":"code","source":"# Setting 'CustomerID' column as index and assigning it to a new dataframe\ndf_customer = customer_data_cleaned.set_index('CustomerID')\n\n# Standardize the data (excluding the cluster column)\nscaler = StandardScaler()\ndf_customer_standardized = scaler.fit_transform(df_customer.drop(columns=['cluster'], axis=1))\n\n# Create a new dataframe with standardized values and add the cluster column back\ndf_customer_standardized = pd.DataFrame(df_customer_standardized, columns=df_customer.columns[:-1], index=df_customer.index)\ndf_customer_standardized['cluster'] = df_customer['cluster']\n\n# Calculate the centroids of each cluster\ncluster_centroids = df_customer_standardized.groupby('cluster').mean()\n\n# Function to create a radar chart\ndef create_radar_chart(ax, angles, data, color, cluster):\n    # Plot the data and fill the area\n    ax.fill(angles, data, color=color, alpha=0.4)\n    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')\n\n    # Add a title\n    ax.set_title(f'Cluster {cluster}', size=20, color=color, y=1.1)\n\n# Set data\nlabels=np.array(cluster_centroids.columns)\nnum_vars = len(labels)\n\n# Compute angle of each axis\nangles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n\n# The plot is circular, so we need to \"complete the loop\" and append the start to the end\nlabels = np.concatenate((labels, [labels[0]]))\nangles += angles[:1]\n\n# Initialize the figure\nfig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=3)\n\n\n# Create radar chart for each cluster\nfor i, color in enumerate(colors):\n    data = cluster_centroids.loc[i].tolist()\n    data += data[:1]  # Complete the loop\n    create_radar_chart(ax[i], angles, data, color, i)\n\n# Add input data\nax[0].set_xticks(angles[:-1])\nax[0].set_xticklabels(labels[:-1])\n\nax[1].set_xticks(angles[:-1])\nax[1].set_xticklabels(labels[:-1])\n\nax[2].set_xticks(angles[:-1])\nax[2].set_xticklabels(labels[:-1])\n\n# Add a grid\nax[0].grid(color='grey', linewidth=0.5)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n# Initialize the figure\nfig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=1)\n\n# Create radar chart for each cluster\nfor i, color in enumerate(colors):\n    data = cluster_centroids.loc[i].tolist()\n    data += data[:1]  # Complete the loop\n    create_radar_chart(ax, angles, data, color, i)\n\n# Add input data\nax.set_xticks(angles[:-1])\nax.set_xticklabels(labels[:-1])\n\n# Add a grid\nax.grid(color='grey', linewidth=0.5)\n\n# Display the plot\nax.set_title('All Clusters', size=20, color=color, y=1.1)\nplt.tight_layout()\nplt.show()\n\n\n\n#df_customer_standardized['cluster'] = km_model.labels_\n\ncluster_0_mean = df_customer_standardized[df_customer_standardized['cluster'] == 0].drop('cluster', axis=1).mean()\ncluster_1_mean = df_customer_standardized[df_customer_standardized['cluster'] == 1].drop('cluster', axis=1).mean()\ncluster_2_mean = df_customer_standardized[df_customer_standardized['cluster'] == 2].drop('cluster', axis=1).mean()\n\nplt.figure(figsize = (30,10) )\nplt.plot(cluster_0_mean, label='Cluster 0', linestyle='-', linewidth=3)\nplt.plot(cluster_1_mean, label='Cluster 1', linestyle='--', linewidth=3)\nplt.plot(cluster_2_mean, label='Cluster 2', linestyle=':', linewidth=3)\nplt.legend(fontsize=20)\nplt.xticks(fontsize=15, rotation=45)\nplt.yticks(fontsize=15)\nplt.show()","metadata":{"papermill":{"duration":1.51518,"end_time":"2023-09-13T18:57:35.893853","exception":false,"start_time":"2023-09-13T18:57:34.378673","status":"completed"},"tags":[],"id":"f43ebcad","outputId":"0574cdb4-bbf8-44fd-9d07-c86a59672f16","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cluster 0\n\nProfile: __Frequent High-Spenders with a High Rate of Cancellations__\n    \n- Customers in this cluster are high spenders with a very high total spend, and they purchase a wide variety of unique products. These are very likely the wholesale customers.\n- They engage in frequent transactions, but also have a high cancellation frequency and rate.  \n- These customers have a very low average time between purchases, and they tend to shop early in the day (low `Hour` value).  \n\n____\n    \n### Cluster 1\n\nProfile: __Weekday Shoppers__  \n    \n- Customers in this cluster show a moderate level of spending, but their transactions are not very frequent, as indicated by the high `Days_Since_Last_Purchase` and `Average_Days_Between_Purchases`.  \n- These customers do most of their shopping throughout the week.\n\n____\n    \n\n### Cluster 2\n\nProfile: __Weekend Shoppers__  \n\nThese customers are similar to those in Cluster 1, with the below differences:\n- They have a greater tendency to shop during the weekends, as indicated by the very high `Day_of_Week` value.  \n- These customers prefer shopping late in the day, as indicated by the high `Hour` value.\n- These customers purchase a wider variety of products as compared to Cluster 1 customers, but with less frequent purchases than Cluster 1 customers.\n","metadata":{"papermill":{"duration":0.164772,"end_time":"2023-09-13T18:57:36.220301","exception":false,"start_time":"2023-09-13T18:57:36.055529","status":"completed"},"tags":[],"id":"1a2c3834"}},{"cell_type":"markdown","source":"We can now add the clusters to the original dataset (with outliers removed). This will enable us to do analysis on transactions per Customer Segment.","metadata":{"id":"J5ki1nbwqVaC"}},{"cell_type":"code","source":"# Step 1: Extract the CustomerIDs of the outliers and remove their transactions from the main dataframe\noutlier_customer_ids = outliers_data['CustomerID'].astype('float').unique()\ndf_filtered = df[~df['CustomerID'].isin(outlier_customer_ids)]\n\n# Step 2: Ensure consistent data type for CustomerID across both dataframes before merging\ncustomer_data_cleaned['CustomerID'] = customer_data_cleaned['CustomerID'].astype('float')\n\n# Step 3: Merge the transaction data with the customer data to get the cluster information for each transaction\nmerged_data = df_filtered.merge(customer_data_cleaned[['CustomerID', 'cluster']], on='CustomerID', how='inner')\n\nmerged_data.head()","metadata":{"id":"OfWqiZavjswu","outputId":"29cc7de1-be25-428c-8a9e-df71c4c97ef4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompt: horizontal bar chart of top 5 purchases for each cluster\n\nfor i in merged_data['cluster'].unique():\n  top_5_purchases = merged_data[merged_data['cluster'] == i].groupby(['cluster', 'Description'])['Quantity'].sum().nlargest(5)\n\n  top_5_purchases = top_5_purchases.reset_index()\n\n  top_5_purchases.head()\n\n  # Create a horizontal bar chart\n  plt.figure(figsize=(4, 2))\n  plt.barh(top_5_purchases['Description'], top_5_purchases['Quantity'], color='blue')\n  plt.xlabel('Quantity')\n  plt.ylabel('Product ID')\n  plt.title('Top 5 Purchases for Cluster ' + str(i))\n  plt.show()\n","metadata":{"id":"4-dyTIy_bVzv","outputId":"a4b6a24f-f1d8-44d6-c1bb-0abff70e017a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_0 = merged_data[merged_data['cluster'] == 0].groupby(['cluster', 'Description'])['Quantity'].sum().nlargest(1)\n# Extract the Description from the result\ntop_cluster0_purchase = result_0.index.get_level_values('Description').values[0]\n\nresult_1 = merged_data[merged_data['cluster'] == 1].groupby(['cluster', 'Description'])['Quantity'].sum().nlargest(1)\n# Extract the Description from the result\ntop_cluster1_purchase = result_1.index.get_level_values('Description').values[0]\n\nresult_2 = merged_data[merged_data['cluster'] == 2].groupby(['cluster', 'Description'])['Quantity'].sum().nlargest(1)\n# Extract the Description from the result\ntop_cluster2_purchase = result_2.index.get_level_values('Description').values[0]","metadata":{"id":"JCuK7_3s61Dw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"invNo_0 = merged_data[(merged_data['cluster'] == 0) & (merged_data['Description'] == top_cluster0_purchase)]['InvoiceNo']\ninvNo_1 = merged_data[(merged_data['cluster'] == 1) & (merged_data['Description'] == top_cluster1_purchase)]['InvoiceNo']\ninvNo_2 = merged_data[(merged_data['cluster'] == 2) & (merged_data['Description'] == top_cluster2_purchase)]['InvoiceNo']","metadata":{"id":"3EkNeB3tg94m","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"next_likely_3_purchases_0 = merged_data[(merged_data['InvoiceNo'].isin(invNo_0)) & (merged_data['Description'] != top_cluster0_purchase)].groupby(['cluster', 'Description'])['Quantity'].sum().nlargest(3)\nnext_likely_3_purchases_0 = next_likely_3_purchases_0.reset_index()\n# Create a horizontal bar chart\nplt.figure(figsize=(4, 2))\nplt.barh(next_likely_3_purchases_0['Description'], next_likely_3_purchases_0['Quantity'], color='blue')\nplt.xlabel('Quantity')\nplt.ylabel('Product ID')\nplt.title('Likely purchases for those who bought Cluster 0 top product')\n\n\nnext_likely_3_purchases_1 = merged_data[(merged_data['InvoiceNo'].isin(invNo_1)) & (merged_data['Description'] != top_cluster1_purchase)].groupby(['cluster', 'Description'])['Quantity'].sum().nlargest(3)\nnext_likely_3_purchases_1 = next_likely_3_purchases_1.reset_index()\n# Create a horizontal bar chart\nplt.figure(figsize=(4, 2))\nplt.barh(next_likely_3_purchases_1['Description'], next_likely_3_purchases_1['Quantity'], color='blue')\nplt.xlabel('Quantity')\nplt.ylabel('Product ID')\nplt.title('Likely purchases for those who bought Cluster 1 top product')\n\n\nnext_likely_3_purchases_2 = merged_data[(merged_data['InvoiceNo'].isin(invNo_2)) & (merged_data['Description'] != top_cluster2_purchase)].groupby(['cluster', 'Description'])['Quantity'].sum().nlargest(3)\nnext_likely_3_purchases_2 = next_likely_3_purchases_2.reset_index()\n# Create a horizontal bar chart\nplt.figure(figsize=(4, 2))\nplt.barh(next_likely_3_purchases_2['Description'], next_likely_3_purchases_2['Quantity'], color='blue')\nplt.xlabel('Quantity')\nplt.ylabel('Product ID')\nplt.title('Likely purchases for those who bought Cluster 2 top product')\n\n\nplt.show()\n","metadata":{"id":"mf83Iy4zbhxb","outputId":"8008374a-f01c-47b6-bf31-32e67b4eb3cf","trusted":true},"outputs":[],"execution_count":null}]}